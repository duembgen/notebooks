---
layout: post
title:  "SOS Polynomials: Kernel vs. Image, Primal vs. Dual"
date:   2025-08-18 09:27:00 +0000
categories: optimization mathematics
---

Checking if a polynomial is non-negative is a fundamental problem that appears in many areas of engineering and mathematics. While checking for non-negativity is computationally hard in general, a powerful sufficient condition is to check if the polynomial can be written as a sum of squares (SOS) of other polynomials. This condition is not only tractable—it can be checked by solving a semidefinite program (SDP)—but it is also a key component in a wide range of optimization methods for polynomial systems.

In this post, we'll explore how to formulate the SOS problem as an SDP. We will see that there isn't just one way to do it. We'll look at two primary approaches, often called the "kernel form" and the "image form," and then explore their dual problems. This will give us four different, but related, optimization problems for tackling the same question.

### A Quick Overview

Let's start with a bird's-eye view of the four formulations we'll discuss. Each is a different way of looking at the same underlying problem of finding an SOS decomposition for a polynomial.

| Formulation Name       | Variables                    | Key Idea                                                            |
| :--------------------- | :--------------------------- | :------------------------------------------------------------------ |
| **Primal Kernel (K-P)**  | Matrix **X**                 | Enforce coefficient matching as linear constraints on **X**.        |
| **Primal Image (I-P)**   | Slack variables **s**        | Parameterize the space of all possible Gram matrices for the polynomial. |
| **Dual Kernel (D-K)**    | Dual multipliers **λ**       | Find a linear combination of basis matrices that is positive semidefinite. |
| **Dual Image (D-I)**     | Matrix **X** (a moment matrix) | Find a certificate **X** that satisfies linear moment constraints.   |

Now, let's dive into the details.

1.  [The Primal Problem: Kernel and Image Forms](#the-primal-problem-kernel-and-image-forms)
2.  [The Dual Problem: Two More Perspectives](#the-dual-problem-two-more-perspectives)
3.  [Connecting the Dots and Correcting the Math](#connecting-the-dots-and-correcting-the-math)
4.  [Conclusion](#conclusion)

### The Primal Problem: Kernel and Image Forms

A polynomial $p(x)$ is a sum of squares if and only if it can be written in the form
$$ p(x) = \mathbf{v}(x)^T \mathbf{X} \mathbf{v}(x) $$
for some positive semidefinite matrix $\mathbf{X} \succeq 0$, called the Gram matrix, and where $\mathbf{v}(x)$ is a vector of monomials.

Our goal is to find such a matrix $\mathbf{X}$. Let's consider a simple example to make things concrete: a univariate polynomial of degree 4.
$$ p(x) = p_0 + p_1x + p_2x^2 + p_3x^3 + p_4x^4 $$
The monomial basis vector is $\mathbf{v}(x) = [1, x, x^2]^T$. The Gram matrix $\mathbf{X}$ will be a $3 \times 3$ symmetric matrix. The condition $p(x) = \mathbf{v}(x)^T \mathbf{X} \mathbf{v}(x)$ can be expanded and written as an inner product:
$$ p(x) = \langle \mathbf{X}, \mathbf{v}(x)\mathbf{v}(x)^T \rangle $$
By matching the coefficients of the powers of $x$ on both sides, we get a system of linear equations that our matrix $\mathbf{X}$ must satisfy.

#### The Kernel Form

The most direct approach is to write these linear equations as constraints in an optimization problem. This is the **kernel form**, as it describes the feasible set as the kernel of a linear operator.

For our example, matching coefficients yields:
*   $p_0 = X_{00}$
*   $p_1 = 2X_{01}$
*   $p_2 = 2X_{02} + X_{11}$
*   $p_3 = 2X_{12}$
*   $p_4 = X_{22}$

This gives us a feasibility problem in the primal kernel form:

**(K-P) The Primal Kernel Problem**
$$
\begin{align}
\text{find} \quad & \mathbf{X} \\
\text{s.t.} \quad & \langle \mathbf{X}, \mathbf{A}_i \rangle = p_i, \quad i=0, \dots, 4 \\
& \mathbf{X} \succeq 0
\end{align}
$$
Here, the matrices $\mathbf{A}_i$ are cleverly chosen to pick out the right elements of $\mathbf{X}$ to match the coefficients $p_i$. This is a standard formulation for an SDP.

#### The Image Form

An alternative is the **image form**. Instead of defining the valid matrices $\mathbf{X}$ by what constraints they must satisfy, we provide a direct parameterization for any valid $\mathbf{X}$.

Notice that our system of 5 linear equations has 6 variables (the unique entries of the symmetric matrix $\mathbf{X}$). This means there is one degree of freedom. We can introduce a "slack" variable, let's call it $s$, to parameterize all possible solutions. A bit of algebra shows that any valid Gram matrix $\mathbf{X}$ can be written as:
$$
\mathbf{X} =
\begin{bmatrix}
p_0 & p_1/2 & -s/2 \\
p_1/2 & p_2+s & p_3/2 \\
-s/2 & p_3/2 & p_4
\end{bmatrix}
$$
This can be expressed as $\mathbf{X} = \mathbf{Y}(\mathbf{p}) + s \mathbf{B}_1$, where $\mathbf{Y}(\mathbf{p})$ is the matrix with $s=0$ and $\mathbf{B}_1$ is a matrix that adds the slack term. The problem is then to find if there *exists* a slack variable $s$ that makes this matrix positive semidefinite.

**(I-P) The Primal Image Problem**
$$
\begin{align}
\text{find} \quad & s \\
\text{s.t.} \quad & \mathbf{Y}(\mathbf{p}) + s \mathbf{B}_1 \succeq 0
\end{align}
$$
For more complex polynomials, we might have multiple slack variables, leading to a formulation $\mathbf{Y}(\mathbf{p}) + \sum_j s_j \mathbf{B}_j \succeq 0$.

> **A Note on the Scribbles:** The notes ask about "3 variables." For our example, there is only one degree of freedom, hence one slack variable. The mention of three variables likely comes from a different example, perhaps a bivariate polynomial, which can have more complex dependencies between coefficients and thus more degrees of freedom.

### The Dual Problem: Two More Perspectives

Every optimization problem has a dual, which provides deep insights and often alternative solution methods. Let's find the duals of our two primal forms.

#### Dual of the Kernel Form

Starting with the primal kernel problem (K-P), we can form its Lagrangian and derive the dual problem. The dual variables, let's call them $\lambda_i$, correspond to the linear constraints. The resulting dual problem is:

**(D-K) The Dual Kernel Problem**
$$
\begin{align}
\text{find} \quad & \boldsymbol{\lambda} = [\lambda_0, \dots, \lambda_4]^T \\
\text{s.t.} \quad & \sum_{i=0}^4 \lambda_i p_i = 0 \\
& \sum_{i=0}^4 \lambda_i \mathbf{A}_i \succeq 0
\end{align}
$$
For our example, the matrix in the second constraint, $\sum \lambda_i \mathbf{A}_i$, turns out to be a beautiful and structured matrix—a Hankel matrix:
$$
\sum_{i=0}^4 \lambda_i \mathbf{A}_i =
\begin{bmatrix}
\lambda_0 & \lambda_1 & \lambda_2 \\
\lambda_1 & \lambda_2 & \lambda_3 \\
\lambda_2 & \lambda_3 & \lambda_4
\end{bmatrix} \succeq 0
$$
*(Note: There appears to be a small error in the scribbled notes on page 4 regarding the structure of this matrix; the correct form is a standard Hankel matrix as shown above).*

#### Dual of the Image Form

Now, let's find the dual of the primal image problem (I-P). The dual variable here will be a matrix, which we'll call $\mathbf{X}$ (as it lives in the same space as our original primal variable). The derivation leads to the following formulation:

**(D-I) The Dual Image Problem**
$$
\begin{align}
\text{find} \quad & \mathbf{X} \\
\text{s.t.} \quad & \langle \mathbf{Y}(\mathbf{p}), \mathbf{X} \rangle = 0 \\
& \langle \mathbf{B}_j, \mathbf{X} \rangle = 0 \quad \text{for all } j \\
& \mathbf{X} \succeq 0
\end{align}
$$
This dual problem looks for a positive semidefinite matrix $\mathbf{X}$ that is orthogonal to our basis matrices $\mathbf{B}_j$ and our initial matrix $\mathbf{Y}(\mathbf{p})$. This matrix $\mathbf{X}$ can be interpreted as a matrix of moments.

### Connecting the Dots and Correcting the Math

We now have four related problems. An important observation from the notes is that the primal image form (I-P) is **not** the dual of the primal kernel form (K-P). They are two distinct primal formulations, which in turn have their own distinct duals.

The notes end with an interesting puzzle: "What went wrong here?" This question arises after an attempt to analyze the dual image problem (D-I). The analysis plugs in a specific rank-one matrix $\mathbf{X} = \mathbf{v}(x)\mathbf{v}(x)^T$ and shows that for this choice, the constraints of (D-I) imply that $p(x)=0$.

The mistake in this reasoning is subtle. The variable $\mathbf{X}$ in the dual problem (D-I) is a general positive semidefinite matrix, not necessarily a rank-one matrix of the form $\mathbf{v}(x)\mathbf{v}(x)^T$. The problem asks if there exists *any* $\mathbf{X} \succeq 0$ satisfying the linear constraints. If the polynomial $p(x)$ is indeed a sum of squares, then strong duality guarantees that such a matrix $\mathbf{X}$ exists. This matrix acts as a "certificate" of non-negativity. The fact that a specific, restrictive choice for $\mathbf{X}$ leads to a contradiction doesn't invalidate the formulation; it simply means that the certificate is not of that restrictive form.

### Conclusion

The problem of determining if a polynomial is a sum of squares is a cornerstone of modern optimization. By exploring the kernel and image formulations and their duals, we gain a richer understanding of the geometry of the problem.

*   The **Primal Kernel (K-P)** and **Dual Image (D-I)** forms both optimize over a matrix $\mathbf{X}$, but with different constraints and objectives.
*   The **Primal Image (I-P)** and **Dual Kernel (D-K)** forms optimize over a set of scalar variables ($s_j$ or $\lambda_i$).

The choice between them can have practical consequences. As hinted in the notes, for a polynomial with few variables but high degree (small $n$, large $d$), the kernel form is often more efficient as it avoids parameterizing a potentially high-dimensional affine space. Conversely, for many variables and low degree (large $n$, small $d$), the image form can be simpler as there are fewer, or even no, slack variables. Understanding all four perspectives allows us to choose the best tool for the job and appreciate the beautiful symmetry between satisfying constraints and parameterizing solutions.
